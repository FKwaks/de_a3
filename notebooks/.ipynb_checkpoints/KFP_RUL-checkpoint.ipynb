{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "import kfd.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(raw_data_path: str) -> str:\n",
    "    '''Hier moet de data import komen'''\n",
    "    import pandas as pd\n",
    "    \n",
    "    #raw data import\n",
    "    data = pd.read_parquet(raw_data_path)\n",
    "    \n",
    "    data.columns = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21', 'NA', 'NA']\n",
    "    del data['NA']\n",
    "\n",
    "    # Creating and adding the RUL to the dataframe\n",
    "    RUL_list = []\n",
    "    for engine in set(data['engine_id']):\n",
    "        max_cycle = data.loc[data['engine_id'] == engine].cycle.max()\n",
    "    \n",
    "        for cycle in list(data.loc[data['engine_id'] == engine].cycle):\n",
    "            RUL_list.append(max_cycle - cycle +1)    \n",
    "        \n",
    "\n",
    "    data.insert(2, 'RUL', RUL_list)\n",
    "    data.to_parquet(cleaned_data_path, compression='GZIP')\n",
    "    \n",
    "    return cleaned_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clean_data_op = comp.create_component_from_func(\n",
    "    get_clean_data, output_component_file = 'get_clean_data.yaml', packages_to_install['fastparquet', 'fsspec', 'gcfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_processing(raw_data_path: str, feature_data_path: str) -> str:\n",
    "    import pandas as pd\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    data = pd.read_parquet(raw_data_path)\n",
    "    \n",
    "    engine = data.iloc[:,0].to_list()\n",
    "    cycle = data.iloc[:,1].to_list()\n",
    "\n",
    "    # Clustering the data\n",
    "    X_cluster = data[['setting1', 'setting2', 'setting3']]\n",
    "\n",
    "    # creates the clusters\n",
    "    kmeans = KMeans(n_clusters=3).fit(X_cluster)\n",
    "    data['settings_clusters'] = kmeans.predict(X_cluster)\n",
    "\n",
    "    features = data.columns[3:-1]\n",
    "    for feature in features:\n",
    "        # Creating min, max and delta variables\n",
    "        data['max_' + feature] = data.groupby('engine_id')[feature].cummax()\n",
    "        data['min_' + feature] = data.groupby('engine_id')[feature].cummin()\n",
    "\n",
    "        data['delta_' + feature] = data.groupby('engine_id')[feature].diff()\n",
    "        data['delta_' + feature].fillna(0, inplace=True)\n",
    "\n",
    "    \n",
    "    data.to_parquet(feature_data_path, compression='GZIP')\n",
    "    \n",
    "    print('Created an saved features.')\n",
    "    \n",
    "    return feature_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_processing_op = comp.create_component_from_func(\n",
    "    feature_processing, output_component_file = 'feature_processing.yaml', packages_to_install['fastparquet', 'fsspec', 'gcfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vanilla_gbr(feature_data_path: str, holdout_engine: int) -> None:\n",
    "    import pandas as pd\n",
    "    import _pickle as cPickle\n",
    "    from google.cloud import storage\n",
    "    from urlib.parse import urlparse\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import ensemble\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    data = pd.read_parquet(feature_data_path)\n",
    "    \n",
    "    RUL_df = data.loc[data.engine_id != holdout_engine].iloc[:,2:].copy()\n",
    "    \n",
    "    labels = RUL_df['RUL']\n",
    "    features = RUL_df.iloc[:,1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    gbr_non_opt = ensemble.GradientBoostingRegressor()\n",
    "    gbr_non_opt.fit(X_train, y_train)\n",
    "    \n",
    "    pred_non_opt = gbr_non_opt.predict(X_test)\n",
    "    print('MAE: %s' % metrics.mean_absolute_error(y_test, pred_non_opt))\n",
    "    print('MSE: %s' % metrics.mean_squared_error(y_test, pred_non_opt))\n",
    "    \n",
    "    with open('/tmp/model.pickle', 'wb') as f:\n",
    "        cPickle.dump(gbr_non_opt, f, -1)\n",
    "        \n",
    "    parse = urlparse(url=vanilla_model_store_path, allow_fragments = False)\n",
    "    if parse.path[0] == '/':\n",
    "        model_path = parse.path[1:]\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(parse.netloc)\n",
    "    blob = bucket.blob(model_path)\n",
    "    blob.upload_from_filename('/tmp/model.pickle')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vanilla_gbr_op = comp.create_component_from_func(\n",
    "    train_vanilla_gbr, output_component_file = 'train_vanilla_gbr.yaml', packages_to_install['fastparquet', 'fsspec', 'gcfs', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_tune_train_gbr(feature_data_path: str, tuned_model_store_path: str, metrics_path: str, holdout_engine: int, random_iterations: int, random_params: str) -> str:\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import _pickle as cPickle\n",
    "    from google.cloud import storage\n",
    "    from urlib.parse import urlparse\n",
    "    from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "    from sklearn import ensemble\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    data = pd.read_parquet(feature_data_path)\n",
    "    \n",
    "    RUL_df = data.loc[data.engine_id != holdout_engine].iloc[:,2:].copy()\n",
    "    \n",
    "    labels = RUL_df['RUL']\n",
    "    features = RUL_df.iloc[:,1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    random_grid = json.loads(random_params)\n",
    "    \n",
    "    gbr = ensemble.GradientBoostingRegressor()\n",
    "    gbr_random = RandomizedSearchCV(estimator = gbr, param_distributions = random_grid, n_iter = 10, cv = 3, verbose = 2)\n",
    "    gbr_random.fit(X_train, y_train)\n",
    "\n",
    "    val_pred_random = gbr_random.predict(X_test)\n",
    "    MAE_random = metrics.mean_absolute_error(y_test, val_pred_random)\n",
    "    MSE_random = metrics.mean_squared_error(y_test, val_pred_random)\n",
    "    print('MAE: %s' % MAE_random)\n",
    "    print('MSE: %s' % MSE_random)\n",
    "    \n",
    "    metrics = {\n",
    "        'metrics':\n",
    "        [{\n",
    "            'name' : 'tuned-mae-score'\n",
    "            'numbervalue' : MAE_score\n",
    "        },\n",
    "        {\n",
    "            'name' : 'tuned-mse-score'\n",
    "            'numbervalue' : MSE_score\n",
    "        },\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    temp_metrics_path = '/mlpipeline-metrics.json'\n",
    "    temp_model_path = '/tmp/model.pickle'\n",
    "    \n",
    "    with open(temp_metrics_path, 'wb') as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    with open(temp_model_path, 'wb') as f:\n",
    "        cPickle.dump(gbr_random.best_estimator_, f, -1)\n",
    "        \n",
    "    parse = urlparse(url=tuned_model_store_path, allow_fragments = False)\n",
    "    if parse.path[0] == '/':\n",
    "        model_path = parse.path[1:]\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(parse.netloc)\n",
    "    blob = bucket.blob(tuned_model_store_path)\n",
    "    blob.upload_from_filename(model_path)\n",
    "    blob = bucket.blob(metrics_path)\n",
    "    blob.upload_from_filename(temp_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_tune_train_gbr_op = comp.create_component_from_func(\n",
    "    hyp_tune_train_gbr, output_component_file = 'hyp_tune_train_gbr.yaml', packages_to_install['fastparquet', 'fsspec', 'gcfs', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [150, 250, 300, 400]\n",
    "max_depth = [5, 10, 50, 100, None]\n",
    "max_features = ['auto', 'sqrt']\n",
    "min_samples_split= [2, 3, 5, 10]\n",
    "min_samples_leaf= [1, 2, 4]\n",
    "\n",
    "random_grid = {'n_estimators' : n_estimators,\n",
    "               'max_depth' : max_depth,\n",
    "               'max_features' : max_features,\n",
    "               'min_samples_split' : min_samples_split,\n",
    "               'min_samples_leaf' : min_samples_leaf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='RUL gbr'\n",
    "    descriprion='Predicting the Remaining Usefull Lifetime of aircraft engines.'\n",
    ")\n",
    "def RUL_pipeline(raw_data_path, feature_data_path, vanilla_model_store_path, tuned_model_store_path, metrics_path, holdout_engine, random_iterations, random_params):\n",
    "    \n",
    "    get_data_task = get_data_op(raw_data_path)\n",
    "    \n",
    "    feature_processing_task = feature_processing_op(get_data_task.output, feature_data_path)\n",
    "    \n",
    "    train_vanilla_gbr_tasl = train_vanilla_gbr_op(feature_processing_task.output, vanilla_model_store_path, holdout_engine)\n",
    "    \n",
    "    hyp_tune_train_gbr_task = hyp_tune_train_op(feature_processing_task.output, tuned_model_store_path, metrics_path, holdout_engine, random_iterations, random_params).apply()\n",
    "    \n",
    "    arguments = {'raw_data_path': '',\n",
    "                 'cleaned_data_path' : '',\n",
    "                 'feature_data_path' : '',\n",
    "                 'vanilla_model_store_path' : '',\n",
    "                 'tuned_model_store_path' : '',\n",
    "                 'metrics_path': '',\n",
    "                 'holdout_engine': 62,\n",
    "                 'random_iterations': 3,\n",
    "                 'random_params': random_grid\n",
    "    }\n",
    "\n",
    "    \n",
    "client.create_run_from_pipeline_func(RUL_pipeline, arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>engine_id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>RUL</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>...</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>190</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>...</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>189</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>...</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>188</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>...</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   engine_id  cycle  RUL  setting1  setting2  setting3      s1      s2  \\\n",
       "0          1      1  192   -0.0007   -0.0004     100.0  518.67  641.82   \n",
       "1          1      2  191    0.0019   -0.0003     100.0  518.67  642.15   \n",
       "2          1      3  190   -0.0043    0.0003     100.0  518.67  642.35   \n",
       "3          1      4  189    0.0007    0.0000     100.0  518.67  642.35   \n",
       "4          1      5  188   -0.0019   -0.0002     100.0  518.67  642.37   \n",
       "\n",
       "        s3       s4  ...     s12      s13      s14     s15   s16  s17   s18  \\\n",
       "0  1589.70  1400.60  ...  521.66  2388.02  8138.62  8.4195  0.03  392  2388   \n",
       "1  1591.82  1403.14  ...  522.28  2388.07  8131.49  8.4318  0.03  392  2388   \n",
       "2  1587.99  1404.20  ...  522.42  2388.03  8133.23  8.4178  0.03  390  2388   \n",
       "3  1582.79  1401.87  ...  522.86  2388.08  8133.83  8.3682  0.03  392  2388   \n",
       "4  1582.85  1406.22  ...  522.19  2388.04  8133.80  8.4294  0.03  393  2388   \n",
       "\n",
       "     s19    s20      s21  \n",
       "0  100.0  39.06  23.4190  \n",
       "1  100.0  39.00  23.4236  \n",
       "2  100.0  38.95  23.3442  \n",
       "3  100.0  38.88  23.3739  \n",
       "4  100.0  38.90  23.4044  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21', 'NA', 'NA']\n",
    "del data['NA']\n",
    "\n",
    "# Creating and adding the RUL to the dataframe\n",
    "RUL_list = []\n",
    "for engine in set(data['engine_id']):\n",
    "    max_cycle = data.loc[data['engine_id'] == engine].cycle.max()\n",
    "    \n",
    "    for cycle in list(data.loc[data['engine_id'] == engine].cycle):\n",
    "        RUL_list.append(max_cycle - cycle +1)    \n",
    "        \n",
    "\n",
    "data.insert(2, 'RUL', RUL_list)\n",
    "\n",
    "data.head()\n",
    "dataset = data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dataset.iloc[:,0].to_list()\n",
    "cycle = dataset.iloc[:,1].to_list()\n",
    "\n",
    "# Clustering the data\n",
    "X_cluster = data[['setting1', 'setting2', 'setting3']]\n",
    "\n",
    "# creates the clusters\n",
    "kmeans = KMeans(n_clusters=3).fit(X_cluster)\n",
    "data['settings_clusters'] = kmeans.predict(X_cluster)\n",
    "\n",
    "features = data.columns[3:-1]\n",
    "for feature in features:\n",
    "    # Creating min, max and delta variables\n",
    "    data['max_' + feature] = data.groupby('engine_id')[feature].cummax()\n",
    "    data['min_' + feature] = data.groupby('engine_id')[feature].cummin()\n",
    "\n",
    "    data['delta_' + feature] = data.groupby('engine_id')[feature].diff()\n",
    "    data['delta_' + feature].fillna(0, inplace=True)\n",
    "\n",
    "data.to_parquet(feature_path + 'RUL_features.parquet', compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_engine = np.random.randint(1,100)\n",
    "val_data = data.loc[data.engine_id == random_engine].iloc[:,2:].copy()    \n",
    "RUL_df = data.loc[data.engine_id != random_engine].iloc[:,2:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = RUL_df['RUL']\n",
    "features = RUL_df.iloc[:,1:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_non_opt = ensemble.GradientBoostingRegressor()\n",
    "gbr_non_opt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 9.200445326669886\n",
      "MSE: 142.63671398695553\n"
     ]
    }
   ],
   "source": [
    "pred_non_opt = gbr_non_opt.predict(X_test)\n",
    "print('MAE: %s' % metrics.mean_absolute_error(y_test, pred_non_opt))\n",
    "print('MSE: %s' % metrics.mean_squared_error(y_test, pred_non_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [150, 250, 300, 400]\n",
    "max_depth = [5, 10, 50, 100, None]\n",
    "max_features = ['auto', 'sqrt']\n",
    "min_samples_split= [2, 3, 5, 10]\n",
    "min_samples_leaf= [1, 2, 4]\n",
    "\n",
    "random_grid = json.dumps({'n_estimators' : n_estimators,\n",
    "                           'max_depth' : max_depth,\n",
    "                           'max_features' : max_features,\n",
    "                           'min_samples_split' : min_samples_split,\n",
    "                           'min_samples_leaf' : min_samples_leaf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] n_estimators=300, min_samples_split=5, min_samples_leaf=4, max_features=sqrt, max_depth=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "gbr = ensemble.GradientBoostingRegressor()\n",
    "gbr_random = RandomizedSearchCV(estimator = gbr, param_distributions = random_grid, n_iter = 10, cv = 3, verbose = 2)\n",
    "gbr_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(best_param.json, 'w') as outfile:\n",
    "    json.dump(gbr_random.best_params_, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(gbr_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = val_data.iloc[:,1:], val_data.iloc[:,0]\n",
    "\n",
    "val_pred_non_opt = gbr_non_opt.predict(X_val)\n",
    "val_pred_random = gbr_random.predict(X_val)\n",
    "print('MAE: %s' % metrics.mean_absolute_error(y_val val_pred_non_opt))\n",
    "print('MSE: %s' % metrics.mean_squared_error(y_val, val_pred_non_opt))\n",
    "print('MAE: %s' % metrics.mean_absolute_error(y_val, val_pred_random))\n",
    "print('MSE: %s' % metrics.mean_squared_error(y_val, val_pred_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics.mean_absolute_error(y_val val_pred_non_opt)) < metrics.mean_absolute_error(y_val, val_pred_random)):\n",
    "    gbr_best = gbr_random.best_estimator\n",
    "    print('optimized version is best')\n",
    "else:\n",
    "    gbr_best = gbr_non_opt\n",
    "    print('non optimized version is best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tmp_dir + 'model.pickle', 'wb') as f:\n",
    "    cPickle.dump(gbr_best, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket('RUL_prediction')\n",
    "blob = bucket.blob(model_path+'model.pickle')\n",
    "blob.upload_from_filename('/tmp/model.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
